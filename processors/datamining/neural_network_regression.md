![](/media/app/icons/vendors/regressionneuralnet.svg)
# Нейросеть (регрессия)

Решает задачу [регрессии](https://basegroup.ru/community/glossary/regression) - в выходном наборе [нейросеть](https://basegroup.ru/deductor/function/algorithm/neuronet) выдаст прогнозируемое значение для одной или нескольких переменных, зависимых от множества входных параметров.

Перед тем, как производить прогноз, алгоритм обучается на тренировочном наборе данных - обучающей выборке. Каждая строка такой выборки содержит:

*  в полях, обозначаемых как **входные**, - множество входных параметров;

*  в полях, обозначаемых как **выходные**, - соответствующие входным параметрам значения зависимых переменных. 

Технически обучение заключается в нахождении *весов - коэффициентов связей между нейронами*. В процессе обучения нейронная сеть способна выявлять сложные зависимости между входными параметрами и выходными, а также выполнять обобщение. Это значит, что в случае успешного обучения сеть сможет вернуть верный результат на основании данных, которые отсутствовали в обучающей выборке, а также неполных и/или «зашумленных», частично искажённых данных. Для обучения используется квазиньютоновский [метод Бройдена-Флетчера-Гольдфарба-Шанно](https://ru.wikipedia.org/wiki/Алгоритм_Бройдена_—_Флетчера_—_Гольдфарба_—_Шанно) с ограниченным использованием памяти L-BFGS.

В задаче регрессии (в отличии от [задачи классификации](/app/processors/data_mining/neural_network_classification.md)) **выходными** могут быть только поля с непрерывным [видом данных](/app/glossary/datatype.md). Вид данных входных полей не регламентируется. 

----

**Примечание:** Для каждого непрерывного параметра в структуре нейросети будет создан один вход, в то время как для каждой дискретной – столько входов, сколько у данного параметра имеется различных уникальных значений.

----


## Порты

### Вход

* ![](/media/app/icons/ports/output_table_inactive.svg). 

#### Требования к принимаемым данным

Поля входного набора, которые будут использоваться в качестве **входных** или **выходных**, не должны содержать пропущенные значения. Если это требование не выполнено, то в момент активации узла будет выдана ошибка.

### Выход


*  ![](/media/app/icons/ports/output_table_inactive.svg).

*  ![](/media/app/icons/ports/output_variable_inactive.svg) **Показатели качества модели**.


## Мастер настройки

### Шаг 1. Назначение входных столбцов

На первом этапе необходимо задать [назначение](app/glossary/datasetfieldoptions#назначение) полей входного набора данных. 
Для каждого из полей можно выбрать один из вариантов назначения: 

    * ![](/media/app/processors/used_1.svg) **Входное** - поле содержит значения одного из входных параметров;
    * ![](/media/app/processors/usagetype_18-01.svg) **Выходное** - поле содержит значения классов;
    * ![](/media/app/processors/substitution-03.svg) **Неиспользуемое** - поле не участвует в обработке. Устанавливается по умолчанию для прочих полей.

### Шаг 2. Настройка параметров нейросети

#### Структура нейросети

*  Количество скрытых слоев - предоставляется выбор из списка:
    * Без скрытых слоев;
    * Один скрытый слой (используется по умолчанию);
    * Два скрытых слоя.

*  Количество нейронов в первом скрытом слое - целое число >= 1 (по умолчанию = 10);

*  Количество нейронов во втором скрытом слое - целое число >= 1 (по умолчанию = 10).

#### Ограничение на значение выходов

Тип ограничения определяет форму активационной функции выходного слоя нейросети:

*  Нет - линейная;

*  Интервал - гиперболический тангенс;

*  Только снизу - усеченная снизу экспонента;

*  Только сверху - усеченная сверху экспонента.

Границы ограничения определяются параметрами:

*  Верхняя граница

*  Нижняя граница
 
#### Параметры обучения

*  Количество рестартов - число попыток обучения нейросети (на одном и том же наборе), выполняемых из случайных начальных значений весов нейросети. По завершении всех рестартов выбирается сеть, которая обеспечивает наименьшую среднеквадратическую ошибку на обучающем множестве. Целое число >= 1 (по умолчанию = 10);

*  Параметр регуляризации - степень зависимости весов сети друг от друга. Чем больше эта зависимость, тем сильнее будет влияние одного входного параметра на другие. Регуляризация позволяет снизить эффективное число степеней свободы модели, избежав тем самым переобучения. Предоставляется выбор из следующих вариантов:
    * Отсутствует (0);
    * Очень слабая (0,00001);
    * Слабая (0,001). Используется по умолчанию;
    * Средняя (0,1);
    * Сильная (100);
    * Очень сильная (10000).

*  Продолжить обучение - установление данного флага позволяет начать переобучение модели не со случайных значений весов нейросети, а с полученных при последнем обучении. При этом параметр «Количество рестартов» игнорируется.
 
#### Критерии останова

Обучение сети происходит итерационно. При каждой итерации считывается весь обучающий набор данных и изменяются веса нейросети. Этот процесс продолжается пока относительные изменения весов не станут меньше заданного порога или количество итераций не превысит заданной величины.


*  Порог минимального изменения весов - если на очередном шаге обучения относительное изменение нормы вектора весов становится меньше порога, то обучение останавливается. По умолчанию = 0,01;

*  Максимальное количество эпох - максимальное количество итераций обучения алгоритма. Этот параметр по умолчанию отключен. Если процесс обучения необходимо ограничить по времени, в этом случае он остановится после заданного количества эпох, даже если обучение еще не пришло к оптимальной точке, т.е. не достигнут порог минимального изменения весов.
